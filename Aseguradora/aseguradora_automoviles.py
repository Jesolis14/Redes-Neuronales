# -*- coding: utf-8 -*-
"""Copia de EJERCICIO: Aseguradora_Automoviles.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ous49PbG9QF8nCaG_AkgZ_A8aBnO2g8M

#Clases desbalanceadas

Dataset

https://www.kaggle.com/datasets/arashnic/imbalanced-data-practice?select=aug_train.csv

incluir


* Manejo de datos faltantes
* Manejo de datos desbalanceados
* 驴hay outliers?
* estandarizaci贸n de datos
* Keras-tuner, usar conjunto de validaci贸n
  * construir modelo con los mejores par谩metros

  * graficar loss y acc de train y validaci贸n
  * evaluar con X_test
  * obtener matriz de confusi贸n
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler, RobustScaler
import tensorflow as tf
import keras
from keras.models import Sequential
from sklearn import preprocessing
from keras.layers import Dense, Input
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import  ConfusionMatrixDisplay
import keras_tuner as kt

"""NOTA: Nos limitaremos a trabajar con el **aug_train.zip**, porque en el test no viene la soluci贸n con la que vamos a comparar.

#Dataset  
"""

dataset1 = pd.read_csv('aug_train.csv')

dataset = dataset1.copy()

dataset.shape

unique_elements, count_elements = np.unique(dataset.Response,return_counts=True)
print(unique_elements)
print(count_elements)
print(f"Hay un {100*count_elements[0]/count_elements.sum()}% muestras de la clase 0 (no contrata el seguro)")
print(f"Hay un {100*count_elements[1]/count_elements.sum()}% muestras de la clase 1 (contrata el seguro)")

fig1,ax1=plt.subplots()
ax1.pie( count_elements, labels=unique_elements, autopct='%1.1f%%',shadow=True,startangle=90 )
plt.savefig('pie')
plt.close()

"""#Limpieza del dataset"""

print(f'Tama帽o del set antes de eliminar las filas repetidas: {dataset.shape}')
numRenglones = dataset.shape[0]
dataset.drop_duplicates(inplace=True)
print(f'Tama帽o del set despu茅s de eliminar las filas repetidas: {dataset.shape}, se eliminaron {numRenglones-dataset.shape[0]} renglones')

dataset.describe()

dataset = dataset.drop('id', axis=1)

dataset

Q1 = dataset['Annual_Premium'].quantile(0.25)
Q3 = dataset['Annual_Premium'].quantile(0.75)
IQR = Q3 - Q1

limite_inferior = Q1 - 1.5 * IQR
limite_superior = Q3 + 1.5 * IQR
outliers = dataset[(dataset['Annual_Premium'] < limite_inferior) | (dataset['Annual_Premium'] > limite_superior)]

outliers

y = dataset['Response']
X = dataset.drop('Response', axis=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)

"""#Preprocesamiento

*   codificaci贸n de atributos categ贸ricos
*   驴selecci贸n de caracter铆sticas?
* escalamiento
"""

minmax = MinMaxScaler()
robust = RobustScaler()

gender = pd.get_dummies(X_train['Gender'], prefix='Gender_').astype('int')
X_train = pd.concat([X_train, gender], axis=1).drop('Gender', axis=1)

X_train['Vehicle_Age'] = X_train['Vehicle_Age'].map({'< 1 Year': 0, '1-2 Year': 1, '> 2 Years': 2})

code = pd.get_dummies(X_train['Region_Code'], prefix = 'Code_').astype(int)
X_train = pd.concat([X_train, code], axis=1).drop('Region_Code', axis=1)

policy = pd.get_dummies(X_train['Policy_Sales_Channel'], prefix= 'Policy_').astype(int)
X_train = pd.concat([X_train, policy], axis=1).drop('Policy_Sales_Channel', axis=1)

X_train['Vehicle_Damage'] = X_train['Vehicle_Damage'].apply(lambda x: 1 if x == 'Yes' else 0)

X_train[['Age', 'Vintage']] = minmax.fit_transform(X_train[['Age', 'Vintage']])
X_train[['Annual_Premium']] = robust.fit_transform(X_train[['Annual_Premium']])

gender = pd.get_dummies(X_test['Gender'], prefix='Gender_').astype('int')
X_test = pd.concat([X_test, gender], axis=1).drop('Gender', axis=1)

X_test['Vehicle_Age'] = X_test['Vehicle_Age'].map({'< 1 Year': 0, '1-2 Year': 1, '> 2 Years': 2})

code = pd.get_dummies(X_test['Region_Code'], prefix = 'Code_').astype(int)
X_test = pd.concat([X_test, code], axis=1).drop('Region_Code', axis=1)

policy = pd.get_dummies(X_test['Policy_Sales_Channel'], prefix= 'Policy_').astype(int)
X_test = pd.concat([X_test, policy], axis=1).drop('Policy_Sales_Channel', axis=1)

X_test['Vehicle_Damage'] = X_test['Vehicle_Damage'].apply(lambda x: 1 if x == 'Yes' else 0)

X_test[['Age', 'Vintage']] = minmax.transform(X_test[['Age', 'Vintage']])
X_test[['Annual_Premium']] = robust.transform(X_test[['Annual_Premium']])

X_test = X_test.reindex(columns=X_train.columns, fill_value=0)

X_train_scaler = X_train.copy()
X_test_scaler = X_test.copy()

"""#Buscando el mejor modelo"""

def build_model(hp):
    model = Sequential()
    model.add( Input(  shape=(X_train.shape[1],)) )


    for i in range(hp.Int("num_layers", 1, 3)):
        model.add(
            Dense(
                units      = hp.Int   (f'units_{i}', min_value=5, max_value=10, step=1 ),
                activation = hp.Choice(f"activation_{i}", ["relu", "selu","leaky_relu"]),
            )
        )

    model.add( Dense( 1, activation = 'sigmoid', name = "predictions" ) )

    lr = hp.Choice( 'lr', values=[1e-1, 1e-2, 1e-3, 1e-4] )
    optimizers_dict = {
        "Adam":    keras.optimizers.Adam(learning_rate=lr),
        "SGD":     keras.optimizers.SGD(learning_rate=lr),
        "Adagrad": keras.optimizers.Adagrad(learning_rate=lr)
        }

    hp_optimizers = hp.Choice(
        'optimizer',
        values=[ "SGD", "Adam", "Adagrad"]
        )

    model.compile( optimizer    = optimizers_dict[hp_optimizers],
                    loss      = "binary_crossentropy",
                    metrics   = ['accuracy']
                    )

    return model

build_model(kt.HyperParameters())

tuner = kt.Hyperband(
    build_model,
    objective            = kt.Objective("val_accuracy", "max"),
    executions_per_trial = 1,
    max_epochs           = 50,
    factor               = 3,
    directory            = 'salida',
    project_name         = 'intro_to_HP',
    overwrite            = True
)

hist = tuner.search(X_train_scaler, y_train, validation_split=0.2 )

best_hps = tuner.get_best_hyperparameters()[0]

mi_mejor_modelo = tuner.hypermodel.build(best_hps)
mi_mejor_modelo.summary()

"""##Gr谩ficas

* loss
* accuracy
* matriz de confusi贸n
"""

def plot_hist(hist):
    plt.plot(hist.history["accuracy"])
    plt.plot(hist.history["val_accuracy"])
    plt.title("model accuracy")
    plt.ylabel("accuracy")
    plt.xlabel("epoch")
    plt.legend(["train", "validation"], loc="upper left")
    plt.ylim((0,1.1))
    plt.grid()
    plt.savefig('Accuracy.png')
    plt.close()

def plot_hist_loss(hist):
    plt.plot(hist.history["loss"],'.r')
    plt.plot(hist.history["val_loss"],'*b')
    plt.title("model loss")
    plt.ylabel("loss")
    plt.xlabel("epoch")
    plt.legend(["train", "validation"], loc="upper left")
    plt.grid()
    plt.savefig('Loss.png')

historial = mi_mejor_modelo.fit(X_train_scaler, y_train,validation_split=0.2,  epochs=50,  verbose=0 )
plot_hist(historial)
plot_hist_loss(historial)
mi_mejor_modelo.evaluate(X_test_scaler, y_test)

X_pred = pd.read_csv('aug_test.csv')
X_predict = X_pred.copy()



X_predict = X_predict.drop('id', axis=1, errors='ignore')


gender = pd.get_dummies(X_predict['Gender'], prefix='Gender_').astype(int)
X_predict = pd.concat([X_predict, gender], axis=1).drop('Gender', axis=1)

X_predict['Vehicle_Age'] = X_predict['Vehicle_Age'].map({'< 1 Year': 0, '1-2 Year': 1, '> 2 Years': 2})

code = pd.get_dummies(X_predict['Region_Code'], prefix='Code_').astype(int)
X_predict = pd.concat([X_predict, code], axis=1).drop('Region_Code', axis=1)

policy = pd.get_dummies(X_predict['Policy_Sales_Channel'], prefix='Policy_').astype(int)
X_predict = pd.concat([X_predict, policy], axis=1).drop('Policy_Sales_Channel', axis=1)

X_predict['Vehicle_Damage'] = X_predict['Vehicle_Damage'].apply(lambda x: 1 if x == 'Yes' else 0)

X_predict = X_predict.reindex(columns=X_train_scaler.columns, fill_value=0)

X_predict[['Age', 'Vintage']] = minmax.transform(X_predict[['Age', 'Vintage']])
X_predict[['Annual_Premium']] = robust.transform(X_predict[['Annual_Premium']])

y_pred = mi_mejor_modelo.predict(X_predict)
y_pred_labels = (y_pred > 0.5).astype(int)

df_predictions = pd.DataFrame({
    'id': X_pred['id'],
    'Response': y_pred_labels.flatten()
})

df_predictions.to_csv('Submission.csv')

prediccion_test = mi_mejor_modelo.predict(X_test).ravel()

pred_test = np.zeros(X_test.shape[0])

for id in range(X_test.shape[0]):
    pred_test[id] = np.round( prediccion_test[id])

con = confusion_matrix( y_test , pred_test )
disp = ConfusionMatrixDisplay( confusion_matrix = con,  display_labels = ['No-Venta','Venta'] ).plot()
plt.savefig('Matriz')
plt.close()

