# -*- coding: utf-8 -*-
"""Copia de EJERCICIO: Aseguradora_Automoviles.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ous49PbG9QF8nCaG_AkgZ_A8aBnO2g8M

#Clases desbalanceadas

Dataset

https://www.kaggle.com/datasets/arashnic/imbalanced-data-practice?select=aug_train.csv

incluir


* Manejo de datos faltantes
* Manejo de datos desbalanceados
* ¿hay outliers?
* estandarización de datos
* Keras-tuner, usar conjunto de validación
  * construir modelo con los mejores parámetros

  * graficar loss y acc de train y validación
  * evaluar con X_test
  * obtener matriz de confusión
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler, RobustScaler
import tensorflow as tf
import keras
from keras.models import Sequential
from sklearn import preprocessing
from keras.layers import Dense, Input
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import  ConfusionMatrixDisplay
import keras_tuner as kt

"""NOTA: Nos limitaremos a trabajar con el **aug_train.zip**, porque en el test no viene la solución con la que vamos a comparar.

#Dataset  📋
"""

dataset1 = pd.read_csv('aug_train.csv')

dataset = dataset1.copy()

dataset.shape

unique_elements, count_elements = np.unique(dataset.Response,return_counts=True)
print(unique_elements)
print(count_elements)
print(f"Hay un {100*count_elements[0]/count_elements.sum()}% muestras de la clase 0 (no contrata el seguro)")
print(f"Hay un {100*count_elements[1]/count_elements.sum()}% muestras de la clase 1 (contrata el seguro)")

fig1,ax1=plt.subplots()
ax1.pie( count_elements, labels=unique_elements, autopct='%1.1f%%',shadow=True,startangle=90 )
plt.savefig('pie')
plt.close()

"""#Limpieza del dataset"""

print(f'Tamaño del set antes de eliminar las filas repetidas: {dataset.shape}')
numRenglones = dataset.shape[0]
dataset.drop_duplicates(inplace=True)
print(f'Tamaño del set después de eliminar las filas repetidas: {dataset.shape}, se eliminaron {numRenglones-dataset.shape[0]} renglones')

dataset.describe()

dataset = dataset.drop('id', axis=1)

dataset

Q1 = dataset['Annual_Premium'].quantile(0.25)
Q3 = dataset['Annual_Premium'].quantile(0.75)
IQR = Q3 - Q1

limite_inferior = Q1 - 1.5 * IQR
limite_superior = Q3 + 1.5 * IQR
outliers = dataset[(dataset['Annual_Premium'] < limite_inferior) | (dataset['Annual_Premium'] > limite_superior)]

outliers

y = dataset['Response']
X = dataset.drop('Response', axis=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)

"""#Preprocesamiento

*   codificación de atributos categóricos
*   ¿selección de características?
* escalamiento
"""

minmax = MinMaxScaler()
robust = RobustScaler()

gender = pd.get_dummies(X_train['Gender'], prefix='Gender_').astype('int')
X_train = pd.concat([X_train, gender], axis=1).drop('Gender', axis=1)

X_train['Vehicle_Age'] = X_train['Vehicle_Age'].map({'< 1 Year': 0, '1-2 Year': 1, '> 2 Years': 2})

code = pd.get_dummies(X_train['Region_Code'], prefix = 'Code_').astype(int)
X_train = pd.concat([X_train, code], axis=1).drop('Region_Code', axis=1)

policy = pd.get_dummies(X_train['Policy_Sales_Channel'], prefix= 'Policy_').astype(int)
X_train = pd.concat([X_train, policy], axis=1).drop('Policy_Sales_Channel', axis=1)

X_train['Vehicle_Damage'] = X_train['Vehicle_Damage'].apply(lambda x: 1 if x == 'Yes' else 0)

X_train[['Age', 'Vintage']] = minmax.fit_transform(X_train[['Age', 'Vintage']])
X_train[['Annual_Premium']] = robust.fit_transform(X_train[['Annual_Premium']])

gender = pd.get_dummies(X_test['Gender'], prefix='Gender_').astype('int')
X_test = pd.concat([X_test, gender], axis=1).drop('Gender', axis=1)

X_test['Vehicle_Age'] = X_test['Vehicle_Age'].map({'< 1 Year': 0, '1-2 Year': 1, '> 2 Years': 2})

code = pd.get_dummies(X_test['Region_Code'], prefix = 'Code_').astype(int)
X_test = pd.concat([X_test, code], axis=1).drop('Region_Code', axis=1)

policy = pd.get_dummies(X_test['Policy_Sales_Channel'], prefix= 'Policy_').astype(int)
X_test = pd.concat([X_test, policy], axis=1).drop('Policy_Sales_Channel', axis=1)

X_test['Vehicle_Damage'] = X_test['Vehicle_Damage'].apply(lambda x: 1 if x == 'Yes' else 0)

X_test[['Age', 'Vintage']] = minmax.transform(X_test[['Age', 'Vintage']])
X_test[['Annual_Premium']] = robust.transform(X_test[['Annual_Premium']])

X_test = X_test.reindex(columns=X_train.columns, fill_value=0)

X_train_scaler = X_train.copy()
X_test_scaler = X_test.copy()

"""#Buscando el mejor modelo"""

def build_model(hp):
    model = Sequential()
    model.add( Input(  shape=(X_train.shape[1],)) )


    for i in range(hp.Int("num_layers", 1, 3)):
        model.add(
            Dense(
                units      = hp.Int   (f'units_{i}', min_value=5, max_value=10, step=1 ),
                activation = hp.Choice(f"activation_{i}", ["relu", "selu","leaky_relu"]),
            )
        )

    model.add( Dense( 1, activation = 'sigmoid', name = "predictions" ) )

    lr = hp.Choice( 'lr', values=[1e-1, 1e-2, 1e-3, 1e-4] )
    optimizers_dict = {
        "Adam":    keras.optimizers.Adam(learning_rate=lr),
        "SGD":     keras.optimizers.SGD(learning_rate=lr),
        "Adagrad": keras.optimizers.Adagrad(learning_rate=lr)
        }

    hp_optimizers = hp.Choice(
        'optimizer',
        values=[ "SGD", "Adam", "Adagrad"]
        )

    model.compile( optimizer    = optimizers_dict[hp_optimizers],
                    loss      = "binary_crossentropy",
                    metrics   = ['accuracy']
                    )

    return model

build_model(kt.HyperParameters())

tuner = kt.Hyperband(
    build_model,
    objective            = kt.Objective("val_accuracy", "max"),
    executions_per_trial = 1,
    max_epochs           = 50,
    factor               = 3,
    directory            = 'salida',
    project_name         = 'intro_to_HP',
    overwrite            = True
)

hist = tuner.search(X_train_scaler, y_train, validation_split=0.2 )

best_hps = tuner.get_best_hyperparameters()[0]

mi_mejor_modelo = tuner.hypermodel.build(best_hps)
mi_mejor_modelo.summary()

"""##Gráficas

* loss
* accuracy
* matriz de confusión
"""

def plot_hist(hist):
    plt.plot(hist.history["accuracy"])
    plt.plot(hist.history["val_accuracy"])
    plt.title("model accuracy")
    plt.ylabel("accuracy")
    plt.xlabel("epoch")
    plt.legend(["train", "validation"], loc="upper left")
    plt.ylim((0,1.1))
    plt.grid()
    plt.savefig('Accuracy.png')
    plt.close()

def plot_hist_loss(hist):
    plt.plot(hist.history["loss"],'.r')
    plt.plot(hist.history["val_loss"],'*b')
    plt.title("model loss")
    plt.ylabel("loss")
    plt.xlabel("epoch")
    plt.legend(["train", "validation"], loc="upper left")
    plt.grid()
    plt.savefig('Loss.png')

historial = mi_mejor_modelo.fit(X_train_scaler, y_train,validation_split=0.2,  epochs=50,  verbose=0 )
plot_hist(historial)
plot_hist_loss(historial)
mi_mejor_modelo.evaluate(X_test_scaler, y_test)

X_pred = pd.read_csv('aug_test.csv')
X_predict = X_pred.copy()



X_predict = X_predict.drop('id', axis=1, errors='ignore')


gender = pd.get_dummies(X_predict['Gender'], prefix='Gender_').astype(int)
X_predict = pd.concat([X_predict, gender], axis=1).drop('Gender', axis=1)

X_predict['Vehicle_Age'] = X_predict['Vehicle_Age'].map({'< 1 Year': 0, '1-2 Year': 1, '> 2 Years': 2})

code = pd.get_dummies(X_predict['Region_Code'], prefix='Code_').astype(int)
X_predict = pd.concat([X_predict, code], axis=1).drop('Region_Code', axis=1)

policy = pd.get_dummies(X_predict['Policy_Sales_Channel'], prefix='Policy_').astype(int)
X_predict = pd.concat([X_predict, policy], axis=1).drop('Policy_Sales_Channel', axis=1)

X_predict['Vehicle_Damage'] = X_predict['Vehicle_Damage'].apply(lambda x: 1 if x == 'Yes' else 0)

X_predict = X_predict.reindex(columns=X_train_scaler.columns, fill_value=0)

X_predict[['Age', 'Vintage']] = minmax.transform(X_predict[['Age', 'Vintage']])
X_predict[['Annual_Premium']] = robust.transform(X_predict[['Annual_Premium']])

y_pred = mi_mejor_modelo.predict(X_predict)
y_pred_labels = (y_pred > 0.5).astype(int)

df_predictions = pd.DataFrame({
    'id': X_pred['id'],
    'Response': y_pred_labels.flatten()
})

df_predictions.to_csv('Submission.csv')

prediccion_test = mi_mejor_modelo.predict(X_test).ravel()

pred_test = np.zeros(X_test.shape[0])

for id in range(X_test.shape[0]):
    pred_test[id] = np.round( prediccion_test[id])

con = confusion_matrix( y_test , pred_test )
disp = ConfusionMatrixDisplay( confusion_matrix = con,  display_labels = ['No-Venta','Venta'] ).plot()
plt.savefig('Matriz')
plt.close()

